

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Packaging Applications for the EBI Cloud Portal &mdash; EMBL-EBI Cloud Portal 0.1.1 documentation</title>
  

  
  
  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  

  

  
        <link rel="index" title="Index"
              href="genindex.html"/>
        <link rel="search" title="Search" href="search.html"/>
    <link rel="top" title="EMBL-EBI Cloud Portal 0.1.1 documentation" href="index.html"/>
        <link rel="next" title="Avoid security credentials on git public repository" href="securing_credentials.html"/>
        <link rel="prev" title="Welcome to EMBL-EBI Portal Documentation’s documentation!" href="index.html"/> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html" class="icon icon-home"> EMBL-EBI Cloud Portal
          

          
          </a>

          
            
            
              <div class="version">
                0.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Packaging Applications for the EBI Cloud Portal</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#stack-overview">Stack overview</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#terraform">Terraform</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#the-terraform-lifecycle"><strong>The Terraform lifecycle</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="#planning"><strong>Planning</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="#deploying-a-k-a-applying">**Deploying ****_(a.k.a. applying)_**</a></li>
<li class="toctree-l4"><a class="reference internal" href="#destroying"><strong>Destroying</strong></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#ansible">Ansible</a></li>
<li class="toctree-l3"><a class="reference internal" href="#linking-terraform-and-ansible">Linking Terraform and Ansible</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#the-ebi-cloud-portal-packaging-structure">The EBI Cloud Portal packaging structure</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#cloud-providers"><strong>Cloud providers</strong></a></li>
<li class="toctree-l3"><a class="reference internal" href="#the-general-structure"><strong>The general structure</strong></a></li>
<li class="toctree-l3"><a class="reference internal" href="#where-to-store-your-code"><strong>Where to store your code</strong></a></li>
<li class="toctree-l3"><a class="reference internal" href="#the-manifest-file"><strong>The manifest file</strong></a></li>
<li class="toctree-l3"><a class="reference internal" href="#how-to-organise-your-code-in-the-git-repository"><strong>How to organise your code in the git repository</strong></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#separate-each-cloud-provider"><em>Separate each cloud provider</em></a></li>
<li class="toctree-l4"><a class="reference internal" href="#separate-terraform-and-ansible"><em>Separate Terraform and Ansible</em></a></li>
<li class="toctree-l4"><a class="reference internal" href="#deployment-scripts"><em>Deployment scripts</em></a></li>
<li class="toctree-l4"><a class="reference internal" href="#special-variables"><strong>Special variables</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="#auxiliary-scripts"><em>Auxiliary scripts</em></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#cloud-credentials"><strong>Cloud credentials</strong></a></li>
<li class="toctree-l3"><a class="reference internal" href="#other-configurations-moving-towards-a-profile-concept"><strong>Other configurations (moving towards a profile concept)</strong></a></li>
<li class="toctree-l3"><a class="reference internal" href="#testing-locally"><strong>Testing locally</strong></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#portal-usage">Portal usage</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#configuring-repositories">Configuring repositories</a></li>
<li class="toctree-l3"><a class="reference internal" href="#the-deployment-process-overview">The deployment process overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="#deployment">Deployment</a></li>
<li class="toctree-l3"><a class="reference internal" href="#destroy">Destroy</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="securing_credentials.html">Avoid security credentials on git public repository</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">EMBL-EBI Cloud Portal</a>
        
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>Packaging Applications for the EBI Cloud Portal</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/dev_documentation.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="packaging-applications-for-the-ebi-cloud-portal">
<span id="packaging-applications-for-the-ebi-cloud-portal"></span><h1>Packaging Applications for the EBI Cloud Portal<a class="headerlink" href="#packaging-applications-for-the-ebi-cloud-portal" title="Permalink to this headline">¶</a></h1>
<hr class="docutils" />
<p><em><em>Disclaimer</em></em> <code class="docutils literal"><span class="pre">This</span> <span class="pre">document</span> <span class="pre">is</span> <span class="pre">still</span> <span class="pre">a</span> <span class="pre">work</span> <span class="pre">in</span> <span class="pre">progress.</span></code><code class="docutils literal"><span class="pre">The</span> <span class="pre">EBI</span> <span class="pre">Cloud</span> <span class="pre">Portal</span> <span class="pre">itself</span> <span class="pre">is</span> <span class="pre">still</span> <span class="pre">in</span> <span class="pre">very</span> <span class="pre">active</span> <span class="pre">development.</span></code><code class="docutils literal"><span class="pre">We’re</span> <span class="pre">trying</span> <span class="pre">to</span> <span class="pre">keep</span> <span class="pre">this</span> <span class="pre">document</span> <span class="pre">as</span> <span class="pre">in</span> <span class="pre">sync</span> <span class="pre">as</span> <span class="pre">possible</span> <span class="pre">with</span> <span class="pre">the</span> <span class="pre">application.</span></code><code class="docutils literal"><span class="pre">Any</span> <span class="pre">comment</span> <span class="pre">or</span> <span class="pre">feedback</span> <span class="pre">is</span> <span class="pre">very</span> <span class="pre">appreciated!</span></code></p>
<hr class="docutils" />
<p>The EBI Cloud Portal has been built to provide an App-Store-like experience when deploying applications to internal and public cloud providers, independently from their complexity. This goal is achieved exploiting two open source tools, <a class="reference external" href="https://www.terraform.io/">Terraform</a> and <a class="reference external" href="https://www.ansible.com/">Ansible</a>. While Terraform caters for the infrastructure that must be deployed, Ansible is used to define and apply the configuration of single VMs and orchestrate their interactions. The process of wrapping workloads in an app that will be then understood and deployed by the portal is called packaging.</p>
<div class="section" id="stack-overview">
<span id="stack-overview"></span><h2>Stack overview<a class="headerlink" href="#stack-overview" title="Permalink to this headline">¶</a></h2>
<p>As previously mentioned, the deployment capabilities of the EBI Cloud Portal are based on two Open Source tools, Terraform and Ansible. Let’s explore them a little bit further, then!</p>
<div class="section" id="terraform">
<span id="terraform"></span><h3>Terraform<a class="headerlink" href="#terraform" title="Permalink to this headline">¶</a></h3>
<p>Terraform allows defining the infrastructure an application requires to run in an easily understandable declarative template written in HCL (HashiCorp Configuration Language). VMs, networks, firewalls and storage volumes can easily be defined in a single or multiple files, leaving to Terraform the burden to understand dependencies between all these resources and the order in which they must be created. Due to the intrinsic differences existing between different cloud providers, Terraform is currently unable to provide a single template that is then mapped onto different cloud-specific templates. The person in charge of packaging applications will need to define a Terraform for each cloud provider he or she intends to support the application for. However, this usually comes down to a very reasonable mapping exercise between each cloud provider objects names. Albeit this may be seen as a downside of Terraform, on the other hand, allows exploiting many of the vendor-specific features and services that wouldn’t otherwise be possible to access. At the time of writing, Terraform supports the major public cloud providers (AWS, Google Cloud, Azure, Rackspace, and many <a class="reference external" href="https://www.terraform.io/docs/providers/index.html">more</a>) as well as OpenStack. There are (as always) other cloud orchestrators that are able to deliver similar functionalities, but they are usually bound to a single platform (e.g. AWS CloudFormation or OpenStack Heat).</p>
<div class="section" id="the-terraform-lifecycle">
<span id="the-terraform-lifecycle"></span><h4><strong>The Terraform lifecycle</strong><a class="headerlink" href="#the-terraform-lifecycle" title="Permalink to this headline">¶</a></h4>
<p>Terraform is based on a declarative language, which allows you to define the desired layout of the infrastructure you want to provision in the cloud. The state of each Terraform deployment is tracked in what is called a **state **file, which is basically a list of all the resources Terraform has deployed in the previous run. This behaviour allows the modification of a deployment editing the template as well the redeployment of resources that are no more available. The life-cycle of a Terraform deployment can be divided into mainly three steps: planned, deployed and destroyed.</p>
</div>
<div class="section" id="planning">
<span id="planning"></span><h4><strong>Planning</strong><a class="headerlink" href="#planning" title="Permalink to this headline">¶</a></h4>
<p>Depending on the initial state being an empty or a partially provisioned environment, the operations Terraform will need to perform will be different. For this reason, the software allows listing all the tasks that will be carried out in the following run, comparing the desired state defined in the template and the state file and coming up with a <em>plan</em> that you can revise. This is obtained simply running terraform plan from within the folder containing the terraform template. Keep in mind that if the state file reports that some components are already deployed, Terraform will check if it’s still in place and adjust the plan, if need.</p>
</div>
<div class="section" id="deploying-a-k-a-applying">
<span id="deploying-a-k-a-applying"></span><h4>**Deploying ****_(a.k.a. applying)_**<a class="headerlink" href="#deploying-a-k-a-applying" title="Permalink to this headline">¶</a></h4>
<p>Applying is the operation that deploys a Terraform template to a cloud provider. Terraform will read the template and the state file (if any) figuring out which operations must be carried out to reach convergence, and then start applying them. This process may take a while, depending on the extent of the required changes and their mutual dependencies, but can usually greatly speeded up increasing the <em>parallelism</em>, the number of objects will act on at the same time. Once the deployment is complete, Terraform will output any defined output in the template and exit.</p>
</div>
<div class="section" id="destroying">
<span id="destroying"></span><h4><strong>Destroying</strong><a class="headerlink" href="#destroying" title="Permalink to this headline">¶</a></h4>
<p>After his honourable service, your infrastructure is ready to be torn down or destroyed, following Terraform’s nomenclature. Not violating dependencies is an important factor to consider here, as this might cause errors in the destroy process (i.e. removing a subnetwork while instances are still hooked into it). Terraform wraps all this into an easy to use a single command.</p>
</div>
</div>
<div class="section" id="ansible">
<span id="ansible"></span><h3>Ansible<a class="headerlink" href="#ansible" title="Permalink to this headline">¶</a></h3>
<p>While Terraform provides some features to configure (or <em>localise</em>) VMs after they’re launched, this is limited to uploading bash scripts or a run single command through SSH. Configuring and orchestrating complex deployments do require a fully fledged configuration management system. Countless different software are available to solve this problem, each of them having its own strong and weak points. We’ve eventually chosen Ansible as the configuration management system for the EBI Portal deployments due to its very easy learning curve and to the fact it doesn’t require any agent on the VMs since it only needs an SSH connection to work. On top of that, it’s very easy YAML-based syntax can usually be learned and put into use in a matter of hours, not days.</p>
<p>After a set of resources is created by Terraform, Ansible can take over by applying all the required configuration changes (i.e. install packages or update configuration files). Ansible simplifies the process with a small trade-off: It is good enough to manage and maintain simple to moderately complex infrastructures because it employs a declarative approach to configuration statements. However, it’s important to note that the choice of supporting only Ansible-based deployments from the portal doesn’t imply that applications themselves are forced to use this tool: it’s in fact quite easy to use Ansible to bootstrap a Salt server (or a Puppet master) that is then used by other VMs to configure themselves.</p>
<p>A final comment: from version 2.0, Ansible started offering built-in orchestration features that can be used to create many of the components Terraform currently manages in the EBI Cloud portal deployments. However, at the time of writing the support for most of the cloud providers other than AWS is scant if compared to the set of components Terraform can manage. For this reason, we’re still advocating the use of Terraform to provision the infrastructure when packaging applications for the EBI Cloud Portal.</p>
</div>
<div class="section" id="linking-terraform-and-ansible">
<span id="linking-terraform-and-ansible"></span><h3>Linking Terraform and Ansible<a class="headerlink" href="#linking-terraform-and-ansible" title="Permalink to this headline">¶</a></h3>
<p>Terraform outputs the final state of the deployment in a state file. However, Ansible relies on an inventory file to know to IP addresses of the VMs it needs to talk with and their logical grouping. To bridge this gap, the portal supports <a class="reference external" href="https://github.com/adammck/terraform-inventory">terraform-inventory</a>, a small GO app that is able to parse a Terraform state file and output its content as an Ansible inventory.</p>
</div>
</div>
<div class="section" id="the-ebi-cloud-portal-packaging-structure">
<span id="the-ebi-cloud-portal-packaging-structure"></span><h2>The EBI Cloud Portal packaging structure<a class="headerlink" href="#the-ebi-cloud-portal-packaging-structure" title="Permalink to this headline">¶</a></h2>
<div class="section" id="cloud-providers">
<span id="cloud-providers"></span><h3><strong>Cloud providers</strong><a class="headerlink" href="#cloud-providers" title="Permalink to this headline">¶</a></h3>
<p>The Portal relies on a homogeneous labelling of Cloud Providers to match, for example, deployments with credentials and the cloud-specific code that must be executed each time. We strongly suggest following the labelling schema below to take full advantage of all the features the portal offers.</p>
<p>|Cloud Provider          |Label|
|—|—|
|Amazon Web Services     |AWS|
|Google Compute Platform |GCP|
|Microsoft Azure         |AZURE|
|OpenStack               |OSTACK|</p>
</div>
<div class="section" id="the-general-structure">
<span id="the-general-structure"></span><h3><strong>The general structure</strong><a class="headerlink" href="#the-general-structure" title="Permalink to this headline">¶</a></h3>
<p>Here’s the general structure of a repository hosting a packaged application for the portal.</p>
<p>├ .gitignore├ README.md├ aws│ ├ ansible -&gt; ../gcp/ansible/│ ├ deploy.sh│ ├ destroy.sh│ ├ state.sh│ └ terraform├ gcp│ ├ ansible│ ├ deploy.sh│ ├ destroy.sh│ ├ state.sh│ └ terraform├ manifest.json└ ostack├ ansible -&gt; ../gcp/ansible/├ deploy.sh├ destroy.sh├ state.sh├ terraform└ volume_parser.py</p>
<p>As you can see, there’s a file <code class="docutils literal"><span class="pre">manifest.json</span></code> at the root of it, and then folders storing code for each cloud provider. In this particular repo, the Ansible code is shared among the cloud providers via symlinks, but this is not a strict requirement. Being fully honest, there’s hardly strict requirements at all in the way the Portal consumes applications! Let’s have a more in-depth look, then!</p>
</div>
<div class="section" id="where-to-store-your-code">
<span id="where-to-store-your-code"></span><h3><strong>Where to store your code</strong><a class="headerlink" href="#where-to-store-your-code" title="Permalink to this headline">¶</a></h3>
<p>The code defining an application for the EBI Cloud Portal must be tracked within a git repository publicly clonable over the internet. This is a fundamental requirement, as the way the Portal imports applications in its own registry is cloning such repositories.</p>
<p>Out the many ways, we may have supported this, we eventually chose git repository as this allows to easy to track code changes, keep dev and production deployments separated in different branches, and provides a well-established approach to final users to customize their own deployments forking the original repository.</p>
</div>
<div class="section" id="the-manifest-file">
<span id="the-manifest-file"></span><h3><strong>The manifest file</strong><a class="headerlink" href="#the-manifest-file" title="Permalink to this headline">¶</a></h3>
<p>Each repository defining an application must contain a <em>manifest</em> at its root describing it. This file will be parsed by the EBI Cloud Portal when importing the application to populate the Registry fields. Here’s an example of the manifest file defining an OpenLava cluster application:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s2">&quot;applicationName&quot;</span><span class="p">:</span><span class="s2">&quot;OpenLava cluster&quot;</span><span class="p">,</span>
    <span class="s2">&quot;contactEmail&quot;</span><span class="p">:</span><span class="s2">&quot;dario@ebi.ac.uk&quot;</span><span class="p">,</span>
    <span class="s2">&quot;about&quot;</span><span class="p">:</span><span class="s2">&quot;An OpenLava cluster for AWS, GCP and OpenStack&quot;</span><span class="p">,</span>
    <span class="s2">&quot;version&quot;</span><span class="p">:</span> <span class="s2">&quot;0.1&quot;</span><span class="p">,</span>
    <span class="s2">&quot;inputs&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;nodes&quot;</span><span class="p">],</span>
    <span class="s2">&quot;outputs&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;MASTER_IP&quot;</span><span class="p">],</span>
    <span class="s2">&quot;volumes&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;DATA_DISK_ID&quot;</span><span class="p">],</span>
    <span class="s2">&quot;cloudProviders&quot;</span><span class="p">:</span> <span class="p">[</span>
        <span class="p">{</span> <span class="s2">&quot;cloudProvider&quot;</span><span class="p">:</span><span class="s2">&quot;AWS&quot;</span><span class="p">,</span> <span class="s2">&quot;path&quot;</span><span class="p">:</span><span class="s2">&quot;aws&quot;</span><span class="p">,</span> <span class="s2">&quot;inputs&quot;</span><span class="p">:</span> <span class="p">[</span> <span class="s2">&quot;vpc_id&quot;</span> <span class="p">]</span> <span class="p">},</span>
        <span class="p">{</span> <span class="s2">&quot;cloudProvider&quot;</span><span class="p">:</span><span class="s2">&quot;OSTACK&quot;</span><span class="p">,</span> <span class="s2">&quot;path&quot;</span><span class="p">:</span><span class="s2">&quot;ostack&quot;</span> <span class="p">},</span>
        <span class="p">{</span> <span class="s2">&quot;cloudProvider&quot;</span><span class="p">:</span><span class="s2">&quot;GCP&quot;</span><span class="p">,</span> <span class="s2">&quot;path&quot;</span><span class="p">:</span><span class="s2">&quot;gcp&quot;</span><span class="p">}</span>
   <span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Many of the fields are self-explanatory, but let’s walk through them anyway:</p>
<ul class="simple">
<li><code class="docutils literal"><span class="pre">applicationName</span></code> The name that will be shown in the Portal registry for this application</li>
<li><code class="docutils literal"><span class="pre">contactEmail</span></code> The email address of the person or group maintaining the application</li>
<li><code class="docutils literal"><span class="pre">about</span></code> A (<em>very</em>) brief description of what the application does</li>
<li><code class="docutils literal"><span class="pre">version</span></code> The current version of the application</li>
<li><code class="docutils literal"><span class="pre">inputs</span></code> An array of strings defining the inputs required by the application, in this particular case the number of nodes to be deployed in our OpenLava cluster. Input fields will be shown by the Portal to allow users to customize the deployment behaviour. All the values will then be injected as environment variables when deploying, making them accessible to Terraform and Ansible.</li>
</ul>
<p>Any <code class="docutils literal"><span class="pre">input</span></code> name defined in the manifest will be injected into the environment as <code class="docutils literal"><span class="pre">TF_VAR_input</span></code>. Since Terraform automatically imports environment variables with the <code class="docutils literal"><span class="pre">TF_VAR_</span></code> prefix and maps them to its own internal variables (removing the prefix), this allows to easily wire up the deployment with user inputs. Using our OpenLava deployment as an example, the portal will show an input field named <code class="docutils literal"><span class="pre">nodes</span></code>, and inject the value entered by the user in the environment variable <code class="docutils literal"><span class="pre">TF_VAR_nodes</span></code>, that is the read by Terraform and mapped to its internal variable <code class="docutils literal"><span class="pre">nodes</span></code>. Should an Ansible playbook need to access the same input value, it must look for the <code class="docutils literal"><span class="pre">TF_VAR_input</span></code> environment variable, as no automatic mapping is available in Ansible.</p>
<p><strong>outputs</strong></p>
<p>A very common use case when deploying infrastructure to the cloud is the need to show back to the user some information resulting from the deployment itself, as for example the external IP address of a batch system master node. The portal will scan the output of the Terraform state file looking for the strings defined in this JSON array, and display the result to the user.</p>
<p><strong>volumes</strong></p>
<p>Sometimes, a deployment requires attaching a previously defined volume. For example, some data may be staged in via a GridFTP server on a particular volume, that is then re-attached to an NFS server serving a batch system. The EBI Cloud Portal allows to completely separate the volumes lifecycle from the lifecycle of applications. Adding a volume name (i.e. <code class="docutils literal"><span class="pre">DATA_DISK_ID</span></code> in our previous example) to volumes automatically displays on the deployment card a drop-down menu listing all the volumes deployed through the portal. The id of the selected volume (as provided by the cloud provider, not the portal internal id) is then injected into the deployment process as an environment variable (i.e. <code class="docutils literal"><span class="pre">TF_VAR_DATA_DISK_ID</span></code> in this case).</p>
<p><strong>cloudProviders</strong></p>
<p>This is where the magic happens! This JSON array contains a dictionary (an hash table, following JSON nomenclature) for each cloud provider the application supports. The structure is as follows:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="p">{</span> <span class="s2">&quot;cloudProvider&quot;</span><span class="p">:</span><span class="s2">&quot;AWS&quot;</span><span class="p">,</span> <span class="s2">&quot;path&quot;</span><span class="p">:</span><span class="s2">&quot;aws&quot;</span><span class="p">,</span> <span class="s2">&quot;inputs&quot;</span><span class="p">:</span> <span class="p">[</span> <span class="s2">&quot;vpc_id&quot;</span> <span class="p">]</span> <span class="p">}</span>
</pre></div>
</div>
<p><code class="docutils literal"><span class="pre">cloudProvider</span></code> specifies which cloud provider the dictionary is providing information for, <code class="docutils literal"><span class="pre">path</span></code> is the path to the folder where the code to deploy to the given cloud provider is located, while inputs is an optional JSON array defining cloud-specific inputs (in our example, the <code class="docutils literal"><span class="pre">vpc_id</span></code> to use on AWS).</p>
<p>The <code class="docutils literal"><span class="pre">cloudProvider</span></code> value is also used to pick the right credentials when deploying. At the time of writing, the portal simply looks among the defined credentials and picks the one tagged with the same string (<code class="docutils literal"><span class="pre">AWS</span></code> in this case), so it is important to follow the labelling schema previously mentioned.</p>
</div>
<div class="section" id="how-to-organise-your-code-in-the-git-repository">
<span id="how-to-organise-your-code-in-the-git-repository"></span><h3><strong>How to organise your code in the git repository</strong><a class="headerlink" href="#how-to-organise-your-code-in-the-git-repository" title="Permalink to this headline">¶</a></h3>
<div class="section" id="separate-each-cloud-provider">
<span id="separate-each-cloud-provider"></span><h4><em>Separate each cloud provider</em><a class="headerlink" href="#separate-each-cloud-provider" title="Permalink to this headline">¶</a></h4>
<p>As you’ve learned in the section about the manifest file, the code to deal with each cloud provider must be kept in a separate folder. Even though the naming of these folders is currently left to each individual author, we suggest sticking to the following schema in the “<strong>Cloud providers</strong>” section above.</p>
<p>Following this convention ensures that the repository will be more easily understood by other developers and make the credential matching more reliable.</p>
</div>
<div class="section" id="separate-terraform-and-ansible">
<span id="separate-terraform-and-ansible"></span><h4><em>Separate Terraform and Ansible</em><a class="headerlink" href="#separate-terraform-and-ansible" title="Permalink to this headline">¶</a></h4>
<p>As for the cloud providers, we suggest keeping separate the Terraform and Ansible codebases as this improves much more the readability and maintainability of the repository. Also, it allows for some tricks like sharing the same Ansible code among different cloud providers (symlinks are good!) or using git <a class="reference external" href="https://git-scm.com/book/en/v2/Git-Tools-Submodules">submodules</a> to share code between several deployments.</p>
</div>
<div class="section" id="deployment-scripts">
<span id="deployment-scripts"></span><h4><em>Deployment scripts</em><a class="headerlink" href="#deployment-scripts" title="Permalink to this headline">¶</a></h4>
<p>The EBI Cloud portal is currently unable to directly execute Terraform and Ansible commands, but exploits bash scripts to perform the deployments. Even if some work is currently in progress to move away from that, this is likely to remain the paradigm the portal will follow in the close future. Three deployment scripts are required for each cloud provider: deploy.sh, destroy.sh, state.sh. These must be saved in the root folder of each cloud provider deployment.</p>
</div>
<div class="section" id="special-variables">
<span id="special-variables"></span><h4><strong>Special variables</strong><a class="headerlink" href="#special-variables" title="Permalink to this headline">¶</a></h4>
<p>Aside from the environment variables needed to authenticate against the APIs of the cloud providers, the portal will automatically inject some variables referring to the paths where data can be stored and retrieved, along with the deployment id. Currently, the complete list is as follows:</p>
<p>|Environment variable         |Value|
|—|—|
|PORTAL_APP_REPO_FOLDER       |The path where the application code is stored (e.g. the cloned repo). Only available in the deploy and destroy phase, not when checking the state running state.sh|
|PORTAL_DEPLOYMENTS_ROOT      |The path to the root of the folder storing all the deployment folders|
|PORTAL_DEPLOYMENT_REFERENCE  |The ID assigned to the deployment by the portal|</p>
<p>A very common use case for these variables is to place the Terraform output in the folder belonging to your deployment: this path can be easily obtained joining <code class="docutils literal"><span class="pre">PORTAL_DEPLOYMENTS_ROOT</span></code> and <code class="docutils literal"><span class="pre">PORTAL_DEPLOYMENT_REFERENCE</span></code>, e.g.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="s2">&quot;$PORTAL_DEPLOYMENTS_ROOT&#39;/&#39;$PORTAL_DEPLOYMENT_REFERENCE&#39;/terraform.tfstate&#39;&quot;</span>
</pre></div>
</div>
<p><strong>deploy.sh</strong></p>
<p>This script takes care of deploying the application, and usually consists of at least a Terraform and Ansible call. Here’s a snippet of the current deploy.sh for a GridFTP server on GCP:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>#!/usr/bin/env bash
set -e
# Provisions a GridFTP instance in GCP
# For details about expected inputs and outputs, refer to https://github.com/EMBL-EBI-TSI/gridftp-server
# The script assumes that env vars for authentication with GCP are present.
export TF_VAR_name=&quot;$(awk -v var=&quot;$PORTAL_DEPLOYMENT_REFERENCE&quot; &#39;BEGIN {print tolower(var)}&#39;)&quot;
export KEY_PATH=&quot;${HOME}/.ssh/demo-key.pem&quot;

# Launch provisioning of the VM
terraform apply --state=$PORTAL_DEPLOYMENTS_ROOT&#39;/&#39;$PORTAL_DEPLOYMENT_REFERENCE&#39;/terraform.tfstate&#39; $PORTAL_APP_REPO_FOLDER&#39;/gcp/terraform&#39;

# Start local ssh-agent
eval &quot;$(ssh-agent -s)&quot;
ssh-add $KEY_PATH &amp;&gt; /dev/null

# Get ansible roles
cd gcp/ansible || exit
ansible-galaxy install -r requirements.yml

# Run Ansible
TF_STATE=$PORTAL_DEPLOYMENTS_ROOT&#39;/&#39;$PORTAL_DEPLOYMENT_REFERENCE&#39;/terraform.tfstate&#39; ansible-playbook -i /usr/local/bin/terraform-inventory -u centos -b --tags live deployment.yml &gt; ansible.log 2&gt;&amp;1

# Kill local ssh-agent
eval &quot;$(ssh-agent -k)
</pre></div>
</div>
<p>As you can see, there are a few additional things going on here rather than two simple Terraform and Ansible calls. Again, let’s go step-by-step!</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>#!/usr/bin/env bash
set -e
# Provisions a GridFTP instance in GCP
# For details about expected inputs and outputs, refer to https://github.com/EMBL-EBI-TSI/gridftp-server
# The script assumes that env vars for authentication with GCP are present.
export TF_VAR_name=&quot;$(awk -v var=&quot;$PORTAL_DEPLOYMENT_REFERENCE&quot; &#39;BEGIN {print tolower(var)}&#39;)&quot;
export KEY_PATH=&quot;${HOME}/.ssh/demo-key.pem&quot;
</pre></div>
</div>
<p>This initial block defines the <a class="reference external" href="https://en.wikipedia.org/wiki/Shebang_(Unix)">shebang</a> for the script (<code class="docutils literal"><span class="pre">#!/usr/bin/env</span> <span class="pre">bash</span></code>) and forces the bash script to exit immediately if any command exits with a non-zero status (<code class="docutils literal"><span class="pre">set</span> <span class="pre">-e</span></code>). Then, it exports two environment variables: <code class="docutils literal"><span class="pre">TF_VAR_name</span></code> and <code class="docutils literal"><span class="pre">KEY_PATH</span></code>. The first will automatically be picked up by Terraform and mapped to its internal variable name, eventually causing each resource to be named after the deployment ID (more on this in the next session), while the second allows defining the path to the SSH key to be used to access the VMs.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span># Launch provisioning of the VM
terraform apply --state=$PORTAL_DEPLOYMENTS_ROOT&#39;/&#39;$PORTAL_DEPLOYMENT_REFERENCE&#39;/terraform.tfstate&#39; $PORTAL_APP_REPO_FOLDER&#39;/gcp/terraform&#39;
</pre></div>
</div>
<p>This block is quite obvious: deploy the defined Terraform template to the cloud provider. Keep in mind that the Portal will have already injected the needed credentials in the deployment environment, so you don’t need to care about that.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span># Start local ssh-agent
eval &quot;$(ssh-agent -s)&quot;
ssh-add $KEY_PATH &amp;&gt; /dev/null

# Get ansible roles
cd gcp/ansible || exit
ansible-galaxy install -r requirements.yml

# Run Ansible
TF_STATE=$PORTAL_DEPLOYMENTS_ROOT&#39;/&#39;$PORTAL_DEPLOYMENT_REFERENCE&#39;/terraform.tfstate&#39; ansible-playbook -i /usr/local/bin/terraform-inventory -u centos -b --tags live deployment.yml

# Kill local ssh-agent
eval &quot;$(ssh-agent -k)&quot;
</pre></div>
</div>
<p>This block deals with everything that is needed by Ansible to work. When the Portal launches the deployment script, a new <a class="reference external" href="https://en.wikipedia.org/wiki/Ssh-agent">ssh-agent</a> is spawned and the SSH key to access the VMs is pre-loaded. Then, ansible-galaxy is used to pull all the requirements for the playbook to run (keep in mind that only public repositories will be clonable). Next step, invoking Ansible itself. It’s not a very plain invocation, though:</p>
<ul class="simple">
<li>prefixing the command with <code class="docutils literal"><span class="pre">TF_STATE=...</span></code> tells terraform-inventory where to look for the Terraform state file</li>
<li><code class="docutils literal"><span class="pre">-i</span> <span class="pre">/usr/local/bin/terraform-inventory</span></code> tells Ansible to use terraform-inventory to create the inventory on the flight. Keep in mind that Ansible supports as arguments of the <code class="docutils literal"><span class="pre">-i</span></code> flag both text files containing an inventory and <em>executables returning an inventory.</em></li>
<li><code class="docutils literal"><span class="pre">-u</span> <span class="pre">centos</span> <span class="pre">-b</span></code> force Ansible to use the user centos over ssh and to execute commands with <code class="docutils literal"><span class="pre">sudo</span></code> (b = <a class="reference external" href="http://docs.ansible.com/ansible/become.html">become</a>)</li>
</ul>
<p>The last step is to kill the previously spawned ssh-agent. Deployment (hopefully) done!</p>
<p><strong>destroy.sh</strong></p>
<p>This script is executed by the EBI Cloud Portal to destroy an application. It usually consists of a single Terraform call to destroy the provisioned infrastructure. Here’s an example, again from a GridFTP server.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>#!/usr/bin/env bash
set -e
# Destroys a GridFTP deployment in GCP
# For details about expected inputs and outputs, refer to: https://github.com/EMBL-EBI-TSI/gridftp-server
# The script assumes that env vars for authentication with GCP are already present.

# Export input variable in the bash environment
export TF_VAR_name=&quot;$(awk -v var=&quot;$PORTAL_DEPLOYMENT_REFERENCE&quot; &#39;BEGIN {print tolower(var)}&#39;)&quot;

# Destroy everything
terraform destroy --force --state=$PORTAL_DEPLOYMENTS_ROOT&#39;/&#39;$PORTAL_DEPLOYMENT_REFERENCE&#39;/terraform.tfstate&#39; $PORTAL_APP_REPO_FOLDER&#39;/gcp/terraform&#39;
</pre></div>
</div>
<p>Nothing fancy, right?</p>
<p><strong>state.sh</strong></p>
<p>This script is executed by the Portal immediately after the deployment to grab an updated picture of all the deployed resources. It’s basically a wrapper around the Terraform state command. Here’s the usual example!</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>#!/usr/bin/env bash
set -e
# Get the status of a GridFTP deployment in GCP
# For details about expected inputs and outputs, refer to https://github.com/EMBL-EBI-TSI/gridftp-server
# The script assumes that env vars for authentication with GCP are present.

# Query Terraform state file
terraform show $PORTAL_DEPLOYMENTS_ROOT&#39;/&#39;$PORTAL_DEPLOYMENT_REFERENCE&#39;/terraform.tfstate&#39;
</pre></div>
</div>
</div>
<div class="section" id="auxiliary-scripts">
<span id="auxiliary-scripts"></span><h4><em>Auxiliary scripts</em><a class="headerlink" href="#auxiliary-scripts" title="Permalink to this headline">¶</a></h4>
<p>Depending on the particular needs of each application, you might need auxiliary scripts to carry out the deployment successfully. These can currently be added to any folder within the repo and invoked via the bash scripts. We suggest placing the outputs of these commands (if any) in the deployment folder.</p>
</div>
</div>
<div class="section" id="cloud-credentials">
<span id="cloud-credentials"></span><h3><strong>Cloud credentials</strong><a class="headerlink" href="#cloud-credentials" title="Permalink to this headline">¶</a></h3>
<p>At the moment, the EBI Cloud Portal supports credentials for all cloud providers, as long as these can be provided to Terraform injecting a properly defined environment variable. A user can provide multiple credentials for different cloud providers, but we currently support a <em>single</em> set of credentials for each of them.</p>
<p>Each set of credentials is defined in the portal by three fields:</p>
<ul class="simple">
<li><code class="docutils literal"><span class="pre">Credential</span> <span class="pre">name</span></code> A name for the credentials set</li>
<li><code class="docutils literal"><span class="pre">Cloud</span> <span class="pre">provider</span></code> The cloud provider to which this set of cloud credentials refers to. Please refer to the labelling schema previously mentioned to pick the right label for the cloud provider.</li>
</ul>
<ul class="simple">
<li><code class="docutils literal"><span class="pre">Credentials</span> <span class="pre">fields</span></code>  This field contains a JSON array defining the credentials to be injected into the environment to allow Terraform to authenticate with the cloud provider. Here’s an example of how an OpenStack array looks like:</li>
</ul>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="p">[</span>
    <span class="p">{</span><span class="s2">&quot;key&quot;</span><span class="p">:</span> <span class="s2">&quot;OS_AUTH_URL&quot;</span><span class="p">,</span> <span class="s2">&quot;value&quot;</span><span class="p">:</span><span class="s2">&quot;https://someurl.com:5000/v2.0&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;key&quot;</span><span class="p">:</span> <span class="s2">&quot;OS_TENANT_NAME&quot;</span><span class="p">,</span> <span class="s2">&quot;value&quot;</span><span class="p">:</span> <span class="s2">&quot;tenant_name&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;key&quot;</span><span class="p">:</span> <span class="s2">&quot;OS_USERNAME&quot;</span><span class="p">,</span> <span class="s2">&quot;value&quot;</span><span class="p">:</span> <span class="s2">&quot;username&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;key&quot;</span><span class="p">:</span> <span class="s2">&quot;OS_PASSWORD&quot;</span><span class="p">,</span> <span class="s2">&quot;value&quot;</span><span class="p">:</span> <span class="s2">&quot;password&quot;</span><span class="p">}</span>
<span class="p">]</span>
</pre></div>
</div>
<p>One caveat: since this code will be read by a Java appliance, remember to escape any special character you may have in your credentials. A good example of this is the private key used as part of the GCP authentication: it contains some newline characters (<code class="docutils literal"><span class="pre">\n</span></code>) that will need to be escaped (<code class="docutils literal"><span class="pre">\\n</span></code>).</p>
</div>
<div class="section" id="other-configurations-moving-towards-a-profile-concept">
<span id="other-configurations-moving-towards-a-profile-concept"></span><h3><strong>Other configurations (moving towards a profile concept)</strong><a class="headerlink" href="#other-configurations-moving-towards-a-profile-concept" title="Permalink to this headline">¶</a></h3>
<p>Sometimes injecting credentials are not enough. For example, GCP has the concept of projects, which are a separate compartment in which a single account can be divided into. Terraform needs to know to which compartment resources should be deployed, and this is usually done specifying the project in the <a class="reference external" href="https://www.terraform.io/docs/providers/google/">provider</a>. As the packaged application must be able to deploy itself in any project, this should be provided as an input. However, inputs must be typed in, each time the application is deployed! How can we fix this? Well, here’s the trick: Terraform can also read the project from a dedicated environment variable: <code class="docutils literal"><span class="pre">GOOGLE_PROJECT</span></code>. If we are planning to deploy always to the same project, we can simply add another variable to the credentials JSON array defining the <code class="docutils literal"><span class="pre">GOOGLE_PROJECT</span></code> environment variable, so that it will always be injected when deploying. A whole range of similar problems can be solved via this approach, i.e. feed the id of a shared AWS VPC to the deployments. However, this is currently limited by the fact that we only support a single set of credentials for each cloud provider. Once this limitation will be removed, we’ll revisit the concept of credential sets, possibly moving towards <em>cloud profiles</em>.</p>
</div>
<div class="section" id="testing-locally">
<span id="testing-locally"></span><h3><strong>Testing locally</strong><a class="headerlink" href="#testing-locally" title="Permalink to this headline">¶</a></h3>
<p>Especially at the beginning of the packaging process, it is very useful to test deployments locally. Keep also in mind that if your application fails to deploy in the portal, it might very hard to get a clear reason why that happened without access to the logs (which of course are <em>super-secret!</em>)</p>
<p>So, how to reproduce the Portal behaviour locally? First, you’ll need to install a few dependencies: <a class="reference external" href="https://www.terraform.io/intro/getting-started/install.html">Terraform</a>, <a class="reference external" href="http://docs.ansible.com/ansible/intro_installation.html">Ansible</a> and <a class="reference external" href="https://github.com/adammck/terraform-inventory">terraform-inventory</a> (click on the links to go to their respective “How-to install pages”). Second, you need to replicate the deployment environment. As you should have now understood, the only way the portal interacts with your deployments at the moment is setting <em>environment variables</em>. Reproducing this is very easy, thanks to <a class="reference external" href="https://en.wikipedia.org/wiki/Source_(command)">source</a>!</p>
<p>We can define a small file like this:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1"># Define the three special env vars</span>
<span class="n">export</span> <span class="n">PORTAL_DEPLOYMENTS_ROOT</span><span class="o">=</span><span class="s2">&quot;absolute/path/to/repo&quot;</span>
<span class="n">export</span> <span class="n">PORTAL_DEPLOYMENT_REFERENCE</span><span class="o">=</span><span class="s2">&quot;test_deployment&quot;</span>
<span class="n">export</span> <span class="n">PORTAL_APP_REPO_FOLDER</span><span class="o">=</span><span class="s2">&quot;.&quot;</span>

<span class="c1"># Define the volume id of the volume to be linked to our deployment</span>
<span class="n">export</span> <span class="n">TF_VAR_DATA_DISK_ID</span><span class="o">=</span><span class="s2">&quot;vol-fb65c979&quot;</span>
</pre></div>
</div>
<p>And then simply run <code class="docutils literal"><span class="pre">source</span> <span class="pre">filename</span></code>. This will inject all the variables defined in the file into the bash environment (keep in mind that you’ll need to run again the command if you move to another terminal). At the bare minimum, you’ll need to export the three portal special variables (<code class="docutils literal"><span class="pre">PORTAL_DEPLOYMENTS_ROOT</span></code>, <code class="docutils literal"><span class="pre">PORTAL_DEPLOYMENT_REFERENCE</span></code> and <code class="docutils literal"><span class="pre">PORTAL_APP_REPO_FOLDER</span></code>) plus one variable for each input your application needs (remember to prepend it with <code class="docutils literal"><span class="pre">TF_VAR_</span></code>).</p>
<p>Similarly, you need to source the credentials for the cloud provider you want to interact with. OpenStack allows you to download a pre-populated script to be sourced from its web interface, Horizon (exactly in “Access &amp; Security” tab -&gt; “API Access” subtab -&gt; “Download OpenStack RC file”).</p>
<p>For AWS you’ll need to create your own script to be sourced, here’s an example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="n">export</span> <span class="n">AWS_ACCESS_KEY_ID</span><span class="o">=</span><span class="s2">&quot;your_access_key_id&quot;</span>
<span class="n">export</span> <span class="n">AWS_SECRET_ACCESS_KEY</span><span class="o">=</span><span class="s2">&quot;you_secret_access_key&quot;</span>
</pre></div>
</div>
<p>Finally, for GCP you’ll need to download a JSON file from the <a class="reference external" href="https://console.developers.google.com/">Google Developers Console</a>. Here’s the process step-by-step, as defined by the Terraform documentation for the <a class="reference external" href="https://www.terraform.io/docs/providers/google/">GCP provider</a>:</p>
<ol class="simple">
<li>Log into the Google Developers Console and select a project.</li>
<li>The API Manager view should be selected, click on “Credentials” on the left, then “Create credentials”, and finally “Service account key”.</li>
<li>Select “Compute Engine default service account” in the “Service account” drop-down, and select “JSON” as the key type.</li>
<li>Clicking “Create” will download your credentials.</li>
</ol>
<p>Once you have the file, you can easily define a one-line script to load its content in the appropriate env vars, as follows:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="n">export</span> <span class="n">GOOGLE_CREDENTIALS</span><span class="o">=</span><span class="s2">&quot;`cat path/to/the/json/file.json`&quot;</span>
</pre></div>
</div>
<p>At this point, invoking the various deployment scripts from the root of your repository (i.e. ./gcp/deploy.sh) should just work. **_Happy packaging! _**</p>
</div>
</div>
<div class="section" id="portal-usage">
<span id="portal-usage"></span><h2>Portal usage<a class="headerlink" href="#portal-usage" title="Permalink to this headline">¶</a></h2>
<div class="section" id="configuring-repositories">
<span id="configuring-repositories"></span><h3>Configuring repositories<a class="headerlink" href="#configuring-repositories" title="Permalink to this headline">¶</a></h3>
<p>Before deployments can be made, a user first has to configure portal repositories. These are application definitions which are later used for deployment purposes.</p>
<p>Steps:</p>
<ol class="simple">
<li>User logs into EBI Cloud Portal.</li>
<li>On the Dashboard, the user clicks on “Search Repositories” or selects “Repository” from the left pane menu.</li>
<li>Click on the  “+” button on the right side of the screen.</li>
<li>On the “Add application screen” user pastes public repo URL and clicks “Add”.</li>
<li>The application is added to your repository.</li>
</ol>
</div>
<div class="section" id="the-deployment-process-overview">
<span id="the-deployment-process-overview"></span><h3>The deployment process overview<a class="headerlink" href="#the-deployment-process-overview" title="Permalink to this headline">¶</a></h3>
<p>So, eventually, what are the steps the portal takes every time it needs to deploy or destroy an application? And, how the <code class="docutils literal"><span class="pre">deploy.sh</span></code> and <code class="docutils literal"><span class="pre">destroy.sh</span></code> scripts links into that?</p>
</div>
<div class="section" id="deployment">
<span id="deployment"></span><h3>Deployment<a class="headerlink" href="#deployment" title="Permalink to this headline">¶</a></h3>
<p>Here’s a step-by-step list of every operation the cloud portal performs to deploy an application:</p>
<ul>
<li><p class="first">The user clicks the “Deploy” button on an application in the Portal, after providing all the required inputs and selecting the cloud provider. The web app sends the request to the API.</p>
</li>
<li><p class="first">The selected cloud provider is matched with the credentials in the user profile. If a match is found, they are injected in the deployment environment. If not match is found, the process exits with an error that is reported back to the web app.</p>
</li>
<li><p class="first">Input variables and the volume IDs, if present, are injected into the environment.</p>
</li>
<li><p class="first">The cloud-specific deploy.sh script is executed (e.g. /root/gcp/deploy.sh).</p>
<p>Internally, the <code class="docutils literal"><span class="pre">deploy.sh</span></code> script executes these steps:</p>
<ol class="simple">
<li>Runs Terraform to provision the resources according to the pre-defined template</li>
<li>Runs Ansible to apply the configuration on the provisioned VMs. An Ansible inventory is produced on the fly by terraform-inventory starting from the Terraform state file to feed Ansible with the IPs of the machine it needs to talk to, along with their logical grouping</li>
</ol>
</li>
<li><p class="first">If the deployment script exits with a non-zero status (it fails), the information is sent back to the web app and the process stops. If the deployment script exits with a zero, the process continues</p>
</li>
<li><p class="first">Executes the cloud-specific <code class="docutils literal"><span class="pre">state.sh</span></code> script, and looks for the outputs defined in the manifest (if any)</p>
</li>
<li><p class="first">Reports the outputs (if any) back to the web app</p>
</li>
</ul>
</div>
<div class="section" id="destroy">
<span id="destroy"></span><h3>Destroy<a class="headerlink" href="#destroy" title="Permalink to this headline">¶</a></h3>
<p>The destroy phase is usually much easier - in many cases it only consists of a single Terraform call to tear down the resources. But how this works from the portal perspective?</p>
<p>Again, here’s the list!</p>
<ul>
<li><p class="first">The user clicks the Destroy button on the web application. A request to the API is fired to tear down the deployment</p>
</li>
<li><p class="first">Credentials for the cloud provider hosting the deployment are injected into the environment. If not match is found among the credentials into the user profile, the process exits with an error that is reported back to the web app.</p>
</li>
<li><p class="first">Input variables and the volume IDs, if present, are injected into the environment.</p>
</li>
<li><p class="first">The cloud-specific <code class="docutils literal"><span class="pre">destroy.sh</span></code> script is executed</p>
<p>Internally, the <code class="docutils literal"><span class="pre">destroy.sh</span></code> script executes a single step:</p>
<ol class="simple">
<li>Runs Terraform to destroy  the resources, as they’re reported in the state file</li>
</ol>
<p>In some cases destroying a deployment may require some preliminary steps, e.g. power the VMs off in advance with Ansible. These needs can simply be fulfilled by, for example, using a separate Ansible playbook to be executed before invoking Terraform. It is however imperative that all the unneeded resources are removed at the of the process, as users will not be able to remove them at a later time.</p>
</li>
<li><p class="first">If the destroy script exits with a non-zero status (it fails), an error is displayed by the web app and the process stops. On the contrary, if the destroy script exits with a zero (success), the deployment is removed by the web app and the process concludes.</p>
</li>
</ul>
</div>
</div>
</div>


           </div>
           <div class="articleComments">
            
           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="securing_credentials.html" class="btn btn-neutral float-right" title="Avoid security credentials on git public repository" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="index.html" class="btn btn-neutral" title="Welcome to EMBL-EBI Portal Documentation’s documentation!" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, TSI team, EMBL-EBI.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'0.1.1',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>